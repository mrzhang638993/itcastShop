# canal配置
canal.server.ip=hadoop01
canal.server.port=11111
canal.server.destination=example
canal.server.username=canal
canal.server.password=canal
canal.subscribe.filter=itcast_shop.*

# zookeeper配置
zookeeper.server.ip=hadoop01:2181,hadoop02:2181,hadoop03:2181

# kafka配置
kafka.bootstrap_servers_config=hadoop01:9092,hadoop02:9092,hadoop03:9092
kafka.batch_size_config=1024
#1：表示leader写入成功，就返回，假设leader写完服务器宕机了，还没来得及同步到从节点
#0：异步操作，不管有没有写入成功，都返回，也存在丢失的可能
#-1：当leader写入成功，同时从节点同步成功以后才返回，可以保证不丢失
# 响应模式，1。表示的是leader节点写入成功就返回的。数据没有来得及同步的话，数据存在丢失的
# 0：不管有没有写入成功，都返回操作的。
# -1：表示的是leader节点写入成功，同时从节点同步成功，才返回的。可以保证数据的不丢失的。
kafka.acks=all
# 重试的次数操作。
kafka.retries=0
# kafka批次消费的数据大小。满足了批次的数据大小才会发送数据的。
kafka.batch.size=16384
kafka.client_id_config=itcast_shop_canal_click
# kafka的序列化方式。kafka自带的序列化操作。
kafka.key_serializer_class_config=org.apache.kafka.common.serialization.StringSerializer
# value对应的自定义实现的序列化方式的。写入的是自定义的序列化的方式的。使用protobuf的方式实现自定义的序列化的方式的。
# 需要自定义开发的。
kafka.value_serializer_class_config=cn.itcast.canal.protobuf.ProtoBufSerializer
# 数据写入到kafka的那个指定的topic中的。
kafka.topic=ods_itcast_shop_mysql